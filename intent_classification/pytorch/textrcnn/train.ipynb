{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textrcnn模型](img/textrcnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/365076 [00:00<?, ?it/s]Skipping token b'365076' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|████████████████████████████████████████████████████████████████████████| 365076/365076 [00:41<00:00, 8897.95it/s]\n",
      "D:\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "intent_classification_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "# 训练数据路径\n",
    "train_data = os.path.join(intent_classification_path,'classification_data/classification_data.csv')\n",
    "# 读取数据\n",
    "train_data = pd.read_csv(train_data)\n",
    "# 按字分    \n",
    "tokenize =lambda x: x.split(' ')\n",
    "\n",
    "TEXT = data.Field(\n",
    "                    sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    use_vocab=True,\n",
    "                    pad_token='<pad>',\n",
    "                    unk_token='<unk>',\n",
    "                    batch_first=True,\n",
    "                    fix_length=20)\n",
    "\n",
    "LABEL = data.Field(\n",
    "                    sequential=False,\n",
    "                    use_vocab=False)\n",
    "# 获取训练或测试数据集\n",
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "    fields = [('id', None), ('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    if test: #测试集，不加载label\n",
    "        for text in csv_data['text']:\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else: # 训练集\n",
    "        for text, label in zip(csv_data['text'], csv_data['label']):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "train_examples,train_fields = get_dataset(train_data, TEXT, LABEL)\n",
    "\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "# 预训练数据\n",
    "pretrained_embedding = os.path.join(os.getcwd(), 'sgns.sogou.char')\n",
    "vectors = Vectors(name=pretrained_embedding)\n",
    "# 构建词典\n",
    "TEXT.build_vocab(train, min_freq=1, vectors = vectors)\n",
    "\n",
    "words_path = os.path.join(os.getcwd(), 'words.pkl')\n",
    "with open(words_path, 'wb') as f_words:\n",
    "    pickle.dump(TEXT.vocab, f_words)\n",
    "    \n",
    "BATCH_SIZE = 163\n",
    "# 构建迭代器\n",
    "train_iter = BucketIterator(\n",
    "                            dataset=train,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(TextRCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 这里batch_first=True，只影响输入和输出。hidden与cell还是batch在第2维\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2+embedding_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "         # x :(batch, seq_len) = (163, 20)\n",
    "        # [batch,seq_len,embedding_dim] -> (163, 20, 300)\n",
    "        x = self.embedding(x)\n",
    "        #out=[batch_size, seq_len, hidden_size*2]\n",
    "        #h=[num_layers*2, batch_size, hidden_size]\n",
    "        #c=[num_layers*2, batch_size, hidden_size]\n",
    "        out,(h, c)= self.lstm(x)\n",
    "        # 拼接embedding与bilstm\n",
    "        out = torch.cat((x, out), 2) # [batch_size, seq_len, embedding_dim + hidden_size*2]\n",
    "        # 激活\n",
    "        # out = F.tanh(out)\n",
    "        out = F.relu(out)\n",
    "        # 维度转换 => [batch_size, embedding_dim + hidden_size*2, seq_len]\n",
    "        #out = torch.transpose(out, 1, 2),一维卷积是对输入数据的最后一维进行一维卷积\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = F.max_pool1d(out, out.size(2))\n",
    "        out = out.squeeze(-1) # [batch_size,embedding_dim + hidden_size * 2]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out) # [batch_size, output_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [10/300], Loss: 2.1661\n",
      "iter [20/300], Loss: 1.3261\n",
      "iter [30/300], Loss: 0.7458\n",
      "iter [40/300], Loss: 0.4683\n",
      "iter [50/300], Loss: 0.3658\n",
      "iter [60/300], Loss: 0.3108\n",
      "iter [70/300], Loss: 0.3185\n",
      "iter [80/300], Loss: 0.2496\n",
      "iter [90/300], Loss: 0.1554\n",
      "iter [100/300], Loss: 0.1440\n",
      "iter [110/300], Loss: 0.1080\n",
      "iter [120/300], Loss: 0.0639\n",
      "iter [130/300], Loss: 0.0405\n",
      "iter [140/300], Loss: 0.0455\n",
      "iter [150/300], Loss: 0.0384\n",
      "iter [160/300], Loss: 0.0290\n",
      "iter [170/300], Loss: 0.0266\n",
      "iter [180/300], Loss: 0.0319\n",
      "iter [190/300], Loss: 0.0284\n",
      "iter [200/300], Loss: 0.0308\n",
      "iter [210/300], Loss: 0.0114\n",
      "iter [220/300], Loss: 0.0308\n",
      "iter [230/300], Loss: 0.0133\n",
      "iter [240/300], Loss: 0.0144\n",
      "iter [250/300], Loss: 0.0254\n",
      "iter [260/300], Loss: 0.0089\n",
      "iter [270/300], Loss: 0.0096\n",
      "iter [280/300], Loss: 0.0074\n",
      "iter [290/300], Loss: 0.0082\n",
      "iter [300/300], Loss: 0.0072\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='textrnn')\n",
    "\n",
    "# 训练\n",
    "\n",
    "# 构建model\n",
    "model = TextRCNN(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 2, 16).to(DEVICE)\n",
    "# 利用预训练模型初始化embedding，requires_grad=True，可以fine-tune\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# 训练模式\n",
    "model.train()\n",
    "# 优化和损失\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.1, weight_decay=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.95, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with writer:\n",
    "    for iter in range(300):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            train_text = batch.text\n",
    "            train_label = batch.label\n",
    "            train_text = train_text.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "            out = model(train_text)\n",
    "            loss = criterion(out, train_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (iter+1) % 10 == 0:\n",
    "                    print ('iter [{}/{}], Loss: {:.4f}'.format(iter+1, 300, loss.item()))\n",
    "            #writer.add_graph(model, input_to_model=train_text,verbose=False)\n",
    "            writer.add_scalar('loss',loss.item(),global_step=iter+1)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "            \n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textrcnn模型](img/loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

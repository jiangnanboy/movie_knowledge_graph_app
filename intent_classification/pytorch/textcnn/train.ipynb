{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![textcnn模型](img/textcnn.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "intent_classification_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "# 训练数据路径\n",
    "train_data = os.path.join(intent_classification_path,'classification_data/classification_data.csv')\n",
    "# 读取数据\n",
    "train_data = pd.read_csv(train_data)\n",
    "# 按字分    \n",
    "tokenize =lambda x: x.split(' ')\n",
    "\n",
    "TEXT = data.Field(\n",
    "                    sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    use_vocab=True,\n",
    "                    pad_token='<pad>',\n",
    "                    unk_token='<unk>',\n",
    "                    batch_first=True,\n",
    "                    fix_length=20)\n",
    "\n",
    "LABEL = data.Field(\n",
    "                    sequential=False,\n",
    "                    use_vocab=False)\n",
    "# 获取训练或测试数据集\n",
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "    fields = [('id', None), ('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    if test: #测试集，不加载label\n",
    "        for text in csv_data['text']:\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else: # 训练集\n",
    "        for text, label in zip(csv_data['text'], csv_data['label']):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "train_examples,train_fields = get_dataset(train_data, TEXT, LABEL)\n",
    "\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "# 预训练数据\n",
    "pretrained_embedding = os.path.join(os.getcwd(), 'sgns.sogou.char')\n",
    "vectors = Vectors(name=pretrained_embedding)\n",
    "# 构建词典\n",
    "TEXT.build_vocab(train, min_freq=1, vectors = vectors)\n",
    "\n",
    "words_path = os.path.join(os.getcwd(), 'words.pkl')\n",
    "with open(words_path, 'wb') as f_words:\n",
    "    pickle.dump(TEXT.vocab, f_words)\n",
    "    \n",
    "BATCH_SIZE = 163\n",
    "# 构建迭代器\n",
    "train_iter = BucketIterator(\n",
    "                            dataset=train,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            sort_within_batch=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建分类模型\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, filter_num=100, filter_size=(3,4,5), dropout=0.5):\n",
    "        '''\n",
    "        vocab_size:词典大小\n",
    "        embedding_dim:词维度大小\n",
    "        output_size:输出类别数\n",
    "        filter_num:卷积核数量\n",
    "        filter_size(3,4,5):三种卷积核，size为3,4,5，每个卷积核有filter_num个，卷积核的宽度都是embedding_dim\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # conv2d(in_channel,out_channel,kernel_size,stride,padding),stride默认为1，padding默认为0\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, filter_num,(k, embedding_dim)) for k in filter_size])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(filter_num * len(filter_size), output_size)\n",
    "\n",
    "    '''\n",
    "    以下forward中的卷积和池化计算方式如下：\n",
    "\n",
    "    1.卷积\n",
    "    卷积后的shape公式计算简化为:np.floor((n + 2p - f)/s + 1)\n",
    "    输入shape:(batch, in_channel, hin, win) = (163, 1, 20, 300)，20为句子长度，300为embedding大小\n",
    "    输出shape:\n",
    "    hout=(20 + 2 * 0 - 1 * (3 - 1) - 1)/1 + 1 = 18\n",
    "    wout=(300 + 2 * 0 - 1 * (300 - 1) -1)/1 + 1 = 1\n",
    "    =>\n",
    "    output:(batch, out_channel, hout, wout) = (163, 100, 18, 1)\n",
    "\n",
    "    2.max_pool1d池化\n",
    "    简化公式：np.floor((l + 2p - f)/s + 1)\n",
    "    输入shape:(N,C,L):(163, 100, 18, 1) -> squeeze(3) -> (163, 100, 18)\n",
    "    输出shape:\n",
    "    lout = (18 + 2*0 - 18)/18 +1 = 1 -> (163, 100, 1)\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        # x :(batch, seq_len) = (163, 20)\n",
    "        x = self.embedding(x) # [batch,word_num,embedding_dim] = [N,H,W] -> (163, 20, 300)\n",
    "        x = x.unsqueeze(1) # [batch, channel, word_num, embedding_dim] = [N,C,H,W] -> (163, 1, 20, 300)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # len(filter_size) * (N, filter_num, H) -> 3 * (163, 100, 18)\n",
    "        # MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),stride默认为kernal_size\n",
    "        x = [F.max_pool1d(output,output.shape[2]).squeeze(2) for output in x] # len(filter_size) * (N, filter_num) -> 3 * (163, 100)\n",
    "        x = torch.cat(x, 1) # (N, filter_num * len(filter_size)) -> (163, 100 * 3)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.卷积后的shape计算完整公式：\n",
    "\n",
    "input_shape:(batch,channel,height,width)\n",
    "\n",
    "$Input:(N,C,H_{in},W_{in})$\n",
    "\n",
    "$Output:(N,C,H_{out},W_{out})$\n",
    "\n",
    "$H_{out}=\\lfloor\\frac{H_{in} + 2 * padding[0] - dilation[0] * (kernel_-size[0] - 1) - 1}{stride[0]} + 1\\rfloor$\n",
    "\n",
    "$W_{out}=\\lfloor\\frac{W_{in} + 2 * padding[1] - dilation[1] * (kernel_-size[1] - 1) -1 }{stride[1]} + 1\\rfloor$\n",
    "\n",
    "2.池化max_pool1d计算完整公式：\n",
    "input_shape:(batch, channel, lin)\n",
    "\n",
    "$Input:(N,C,L_{in})$\n",
    "\n",
    "$Output:(N,C,L_{out})$\n",
    "\n",
    "$L_{out}=\\lfloor\\frac{L_{in} + 2 * padding - dilation * (kernel_-size - 1) - 1}{stride} + 1\\rfloor$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [10/300], Loss: 1.7654\n",
      "iter [20/300], Loss: 0.5857\n",
      "iter [30/300], Loss: 0.3431\n",
      "iter [40/300], Loss: 0.1803\n",
      "iter [50/300], Loss: 0.0867\n",
      "iter [60/300], Loss: 0.0373\n",
      "iter [70/300], Loss: 0.0692\n",
      "iter [80/300], Loss: 0.0201\n",
      "iter [90/300], Loss: 0.0205\n",
      "iter [100/300], Loss: 0.0137\n",
      "iter [110/300], Loss: 0.0129\n",
      "iter [120/300], Loss: 0.0134\n",
      "iter [130/300], Loss: 0.0108\n",
      "iter [140/300], Loss: 0.0082\n",
      "iter [150/300], Loss: 0.0075\n",
      "iter [160/300], Loss: 0.0040\n",
      "iter [170/300], Loss: 0.0052\n",
      "iter [180/300], Loss: 0.0040\n",
      "iter [190/300], Loss: 0.0115\n",
      "iter [200/300], Loss: 0.0041\n",
      "iter [210/300], Loss: 0.0027\n",
      "iter [220/300], Loss: 0.0045\n",
      "iter [230/300], Loss: 0.0079\n",
      "iter [240/300], Loss: 0.0049\n",
      "iter [250/300], Loss: 0.0081\n",
      "iter [260/300], Loss: 0.0028\n",
      "iter [270/300], Loss: 0.0027\n",
      "iter [280/300], Loss: 0.0024\n",
      "iter [290/300], Loss: 0.0016\n",
      "iter [300/300], Loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='textcnn')\n",
    "\n",
    "# 训练\n",
    "\n",
    "# 构建model\n",
    "model = TextCNN(len(TEXT.vocab),TEXT.vocab.vectors.shape[1],16).to(DEVICE)\n",
    "# 利用预训练模型初始化embedding，requires_grad=True，可以fine-tune\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# 训练模式\n",
    "model.train()\n",
    "# 优化和损失\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=0.1, weight_decay=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "for iter in range(300):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        train_text = batch.text\n",
    "        train_label = batch.label\n",
    "        train_text = train_text.to(DEVICE)\n",
    "        train_label = train_label.to(DEVICE)\n",
    "        out = model(train_text)\n",
    "        loss = criterion(out, train_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (iter+1) % 10 == 0:\n",
    "                print ('iter [{}/{}], Loss: {:.4f}'.format(iter+1, 300, loss.item()))\n",
    "        writer.add_scalar('loss',loss.item(),global_step=iter+1)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "            \n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "![textcnn模型](img/loss.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}